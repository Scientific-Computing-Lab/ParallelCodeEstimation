{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### System Prompt Crafting\n",
    "\n",
    "This notebook will showcase our attempts at crafting some system prompts and running them through the models we have access to through OpenRouter. Our primary objective is to see how current SoTA LLMs respond to our queries so that we can find out which system prompts give the most consistent responses back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage, SystemMessage, AssistantMessage\n",
    "from autogen_core.model_context import UnboundedChatCompletionContext\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please create a file called '.openrouter-api-key' with your api key and no newline characters\n",
    "with open('./.llm-api-key', 'r') as file:\n",
    "    LLM_API_KEY=file.read()\n",
    "\n",
    "#base_url='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview'\n",
    "base_url='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o-mini/chat/completions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_history_to_json_line(ctxMessages:list):\n",
    "    jsonDict = {'messages':[]}\n",
    "    for msg in ctxMessages:\n",
    "        if type(msg) == SystemMessage:\n",
    "            role = 'system'\n",
    "        elif type(msg) == UserMessage:\n",
    "            role = 'user'\n",
    "        elif type(msg) == AssistantMessage:\n",
    "            role = 'assistant'\n",
    "        else:\n",
    "            assert False, f'Unknown message type: {type(msg)} of {msg}'\n",
    "        content = msg.content\n",
    "\n",
    "        jsonDict['messages'].append({'role':role, 'content':content})\n",
    "\n",
    "    return json.dumps(jsonDict, allow_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def ask_llm_for_roofline_classification(modelName, systemMsg, cudaKernel, temp=1.0):\n",
    "    model_client = AzureOpenAIChatCompletionClient(\n",
    "            model='gpt-4o-mini',\n",
    "            azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "            azure_deployment='gpt-4o-mini',\n",
    "            api_key=LLM_API_KEY,\n",
    "            timeout=60,\n",
    "            temperature=0.1,\n",
    "            api_version='2024-08-01-preview',\n",
    "    )\n",
    "            #model_info = {'vision':False, 'function_calling':True, 'json_output':True, 'model_family':'unknown'}\n",
    "\n",
    "    #print(f'LLM API Key [{LLM_API_KEY}]')\n",
    "    \n",
    "    sys_msg = SystemMessage(content=systemMsg)\n",
    "    #code_msg = UserMessage(source='User', content=f'```{cudaKernel}```')\n",
    "    code_msg = UserMessage(source='User', content=f'```THIS IS A TEST MESSAGE, PLEASE IGNORE!```')\n",
    "    context = UnboundedChatCompletionContext(initial_messages=[sys_msg, code_msg])\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        model_client=model_client,\n",
    "        model_context=context\n",
    "    )\n",
    "\n",
    "    await agent.run()\n",
    "    return await agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scraped kernels: 607\n"
     ]
    }
   ],
   "source": [
    "# let's load up the scraped CUDA kernels\n",
    "jsonFile = './simple-scraped-kernels-CUDA-pruned.json'\n",
    "with open(jsonFile, \"r\") as fp:\n",
    "    cudaKernels = json.load(fp)\n",
    "\n",
    "totalKernels = 0\n",
    "for target in cudaKernels:\n",
    "    kernelNames = target['kernelNames']\n",
    "    totalKernels += len(kernelNames)\n",
    "\n",
    "print('Total scraped kernels:', totalKernels)\n",
    "\n",
    "\n",
    "def write_output_file(filename, contents):\n",
    "    with open(filename, 'w') as fp:\n",
    "        fp.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemMessages = [\n",
    "#    'You are a code analysis assistant that classifies computational kernels into categories based on their performance characteristics. Your task is to provide one of the following classifications: Compute-Bound, Memory-Bound, Balanced, or Other.',\n",
    "    \n",
    "#    'You are a GPU performance analysis expert that classifies computational kernels into categories based on their source code characteristics. Your task is to provide one of the following classifications: Compute-Bound, Memory-Bound, Balanced, or Other.',\n",
    "\n",
    "\n",
    "    '''You are a GPU performance analysis expert that classifies computational kernels into categories based on their source code characteristics. Your task is to provide one of the following performance boundedness classifications: Compute, Bandwidth, Balanced.  A kernel is considered Compute bound if its performance is primarily limited by the number of operations it performs, Bandwidth bound if its performance is primarily limited by the rate at which data can be moved between memory and processing units, and Balanced if the performance is limited roughly equally by compute and memory access.\n",
    "\n",
    "Provide only one word as your response, chosen from the set: ['Compute', 'Bandwidth', 'Balanced'].\n",
    "**Examples:**\n",
    "**Example 1:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000000 {\n",
    "  a[i] = a[i] + b[i];\n",
    "}\n",
    "```\n",
    "Response: Compute\n",
    "\n",
    "**Example 2:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 10 {\n",
    "  load_data(large_array);   //loads from large memory\n",
    "  process_data(large_array); //processes data\n",
    "  store_data(large_array);  //stores back to memory\n",
    "}\n",
    "```\n",
    "Response: Bandwidth\n",
    "\n",
    "**Example 3:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000 {\n",
    "  vector_add(a,b,c);   //process data in situ\n",
    "}\n",
    "//Some smaller data movement but mostly compute.\n",
    "```\n",
    "Response: Compute\n",
    "\n",
    "Now, analyze the following kernel:\n",
    "'''\n",
    "                  ]\n",
    "\n",
    "#models = ['google/gemini-flash-1.5', 'google/gemini-pro']\n",
    "#models = ['google/gemini-flash-1.5']\n",
    "models = ['gpt-4o-mini']\n",
    "\n",
    "#temps = [1.0, 0.8, 0.5, 0.2, 0.0]\n",
    "#temps = [0.1, 0.2, 0.6, 1.2]\n",
    "temps = [0.1]\n",
    "\n",
    "outputFile = 'llm-zero-shot-responses.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatheredData = ''\n",
    "\n",
    "for idx, target in enumerate(cudaKernels):\n",
    "    targetName = target['basename']\n",
    "    kernelNames = target['kernelNames']\n",
    "    kernels = target['kernels']\n",
    "\n",
    "    if len(kernelNames) == 0:\n",
    "        print(f'{targetName} has no found kernels -- skipping')\n",
    "        continue\n",
    "\n",
    "    # for now let's just stop early so we don't waste all our credits\n",
    "    if idx > 1:\n",
    "        break\n",
    "\n",
    "    for kernel in kernelNames:\n",
    "        kernelSrcCode = kernels[kernel]\n",
    "\n",
    "        for sysMsg in systemMessages:\n",
    "            for model in models:\n",
    "                for temp in temps:\n",
    "                    # wait 1 second between invocations, we don't want to get cloudflare banned, again...\n",
    "                    time.sleep(1.0)\n",
    "                    result = await ask_llm_for_roofline_classification(model, sysMsg, kernelSrcCode, temp)\n",
    "                    jsonLResult = chat_history_to_json_line(result)\n",
    "                    gatheredData += f'{jsonLResult}\\n'\n",
    "                    write_output_file(outputFile, gatheredData)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatheredDataJSONL = []\n",
    "with open(outputFile, 'r') as fp:\n",
    "    samples = fp.readlines()\n",
    "    for sample in samples:\n",
    "        sampleData = json.loads(sample)\n",
    "        gatheredDataJSONL.append(sampleData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m gatheredDataJSONL:\n\u001b[1;32m      3\u001b[0m     response \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBandwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check that all the responses are of the desired string\n",
    "for sample in gatheredDataJSONL:\n",
    "    response = sample['messages'][2]['content']\n",
    "    assert response in ['Compute', 'Bandwidth', 'Balanced']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecbench-roofline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
