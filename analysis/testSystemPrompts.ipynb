{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### System Prompt Crafting\n",
    "\n",
    "This notebook will showcase our attempts at crafting some system prompts and running them through the models we have access to through OpenRouter. Our primary objective is to see how current SoTA LLMs respond to our queries so that we can find out which system prompts give the most consistent responses back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage, SystemMessage, AssistantMessage\n",
    "from autogen_core.model_context import UnboundedChatCompletionContext\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "import json\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please create a file called '.openrouter-api-key' with your api key and no newline characters\n",
    "with open('./.llm-api-key', 'r') as file:\n",
    "    LLM_API_KEY=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_history_to_json_line(ctxMessages:list):\n",
    "    jsonDict = {'messages':[]}\n",
    "    for msg in ctxMessages:\n",
    "        if type(msg) == SystemMessage:\n",
    "            role = 'system'\n",
    "        elif type(msg) == UserMessage:\n",
    "            role = 'user'\n",
    "        elif type(msg) == AssistantMessage:\n",
    "            role = 'assistant'\n",
    "        else:\n",
    "            assert False, f'Unknown message type: {type(msg)} of {msg}'\n",
    "        content = msg.content\n",
    "\n",
    "        jsonDict['messages'].append({'role':role, 'content':content})\n",
    "\n",
    "    return json.dumps(jsonDict, allow_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def ask_llm_for_roofline_classification(modelName, systemMsg, cudaKernel, temp=1.0):\n",
    "    model_client = AzureOpenAIChatCompletionClient(\n",
    "            model='gpt-4o-mini',\n",
    "            azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "            azure_deployment='gpt-4o-mini',\n",
    "            api_key=LLM_API_KEY,\n",
    "            timeout=60,\n",
    "            temperature=0.1,\n",
    "            api_version='2024-08-01-preview',\n",
    "    )\n",
    "            #model_info = {'vision':False, 'function_calling':True, 'json_output':True, 'model_family':'unknown'}\n",
    "\n",
    "    #print(f'LLM API Key [{LLM_API_KEY}]')\n",
    "    \n",
    "    sys_msg = SystemMessage(content=systemMsg)\n",
    "    code_msg = UserMessage(source='User', content=f'```{cudaKernel}```')\n",
    "    context = UnboundedChatCompletionContext(initial_messages=[sys_msg, code_msg])\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        model_client=model_client,\n",
    "        model_context=context\n",
    "    )\n",
    "\n",
    "    await agent.run()\n",
    "    return await agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage, SystemMessage\n",
    "from autogen_core.model_context import UnboundedChatCompletionContext\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "        model='gpt-4o-mini',\n",
    "        azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "        azure_deployment='gpt-4o-mini',\n",
    "        api_key=LLM_API_KEY,\n",
    "        timeout=60,\n",
    "        temperature=0.1,\n",
    "        api_version='2024-08-01-preview',\n",
    ")\n",
    "sys_msg = SystemMessage(content='You are a robot and your purpose is to go `beep` and `boop` ONLY.')\n",
    "code_msg = UserMessage(source='User', content=f'```THIS IS A TEST MESSAGE, PLEASE IGNORE!```')\n",
    "context = UnboundedChatCompletionContext(initial_messages=[sys_msg, code_msg])\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    model_context=context\n",
    ")\n",
    "\n",
    "await agent.run()\n",
    "result = await agent._model_context.get_messages()\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scraped kernels: 607\n"
     ]
    }
   ],
   "source": [
    "# let's load up the scraped CUDA kernels\n",
    "jsonFile = './simple-scraped-kernels-CUDA-pruned.json'\n",
    "with open(jsonFile, \"r\") as fp:\n",
    "    cudaKernels = json.load(fp)\n",
    "\n",
    "totalKernels = 0\n",
    "for target in cudaKernels:\n",
    "    kernelNames = target['kernelNames']\n",
    "    totalKernels += len(kernelNames)\n",
    "\n",
    "print('Total scraped kernels:', totalKernels)\n",
    "\n",
    "\n",
    "def write_output_file(filename, contents):\n",
    "    with open(filename, 'w') as fp:\n",
    "        fp.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemMessages = [\n",
    "#    'You are a code analysis assistant that classifies computational kernels into categories based on their performance characteristics. Your task is to provide one of the following classifications: Compute-Bound, Memory-Bound, Balanced, or Other.',\n",
    "    \n",
    "#    'You are a GPU performance analysis expert that classifies computational kernels into categories based on their source code characteristics. Your task is to provide one of the following classifications: Compute-Bound, Memory-Bound, Balanced, or Other.',\n",
    "\n",
    "\n",
    "    '''You are a GPU performance analysis expert that classifies computational kernels into categories based on their source code characteristics. Your task is to provide one of the following performance boundedness classifications: Compute, Bandwidth, Balanced.  A kernel is considered Compute bound if its performance is primarily limited by the number of operations it performs, Bandwidth bound if its performance is primarily limited by the rate at which data can be moved between memory and processing units, and Balanced if the performance is limited roughly equally by compute and memory access.\n",
    "\n",
    "Provide only one word as your response, chosen from the set: ['Compute', 'Bandwidth', 'Balanced'].\n",
    "**Examples:**\n",
    "**Example 1:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000000 {\n",
    "  a[i] = a[i] + b[i];\n",
    "}\n",
    "```\n",
    "Response: Compute\n",
    "\n",
    "**Example 2:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 10 {\n",
    "  load_data(large_array);   //loads from large memory\n",
    "  process_data(large_array); //processes data\n",
    "  store_data(large_array);  //stores back to memory\n",
    "}\n",
    "```\n",
    "Response: Bandwidth\n",
    "\n",
    "**Example 3:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000 {\n",
    "  vector_add(a,b,c);   //process data in situ\n",
    "}\n",
    "//Some smaller data movement but mostly compute.\n",
    "```\n",
    "Response: Compute\n",
    "\n",
    "Now, analyze the following kernel:\n",
    "'''\n",
    "                  ]\n",
    "\n",
    "#models = ['google/gemini-flash-1.5', 'google/gemini-pro']\n",
    "#models = ['google/gemini-flash-1.5']\n",
    "models = ['gpt-4o-mini']\n",
    "\n",
    "#temps = [1.0, 0.8, 0.5, 0.2, 0.0]\n",
    "#temps = [0.1, 0.2, 0.6, 1.2]\n",
    "temps = [0.1]\n",
    "\n",
    "outputFile = 'llm-zero-shot-responses.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m temps:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# wait 1 second between invocations, we don't want to get cloudflare banned, again...\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ask_llm_for_roofline_classification(model, sysMsg, kernelSrcCode, temp)\n\u001b[1;32m     25\u001b[0m     jsonLResult \u001b[38;5;241m=\u001b[39m chat_history_to_json_line(result)\n\u001b[1;32m     26\u001b[0m     gatheredData \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjsonLResult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mask_llm_for_roofline_classification\u001b[0;34m(modelName, systemMsg, cudaKernel, temp)\u001b[0m\n\u001b[1;32m     18\u001b[0m context \u001b[38;5;241m=\u001b[39m UnboundedChatCompletionContext(initial_messages\u001b[38;5;241m=\u001b[39m[sys_msg, code_msg])\n\u001b[1;32m     20\u001b[0m agent \u001b[38;5;241m=\u001b[39m AssistantAgent(\n\u001b[1;32m     21\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[1;32m     23\u001b[0m     model_context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39m_model_context\u001b[38;5;241m.\u001b[39mget_messages()\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/autogen_agentchat/agents/_base_chat_agent.py:136\u001b[0m, in \u001b[0;36mBaseChatAgent.run\u001b[0;34m(self, task, cancellation_token)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages(input_messages, cancellation_token)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39minner_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     output_messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39minner_messages\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:352\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages\u001b[0;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(messages, cancellation_token):\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/autogen_agentchat/agents/_assistant_agent.py:386\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[0;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Generate an inference result based on the current model context.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m llm_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system_messages \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_context\u001b[38;5;241m.\u001b[39mget_messages()\n\u001b[0;32m--> 386\u001b[0m model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    387\u001b[0m     llm_messages, tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tools \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handoff_tools, cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token\n\u001b[1;32m    388\u001b[0m )\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Add the response to the model context.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_context\u001b[38;5;241m.\u001b[39madd_message(AssistantMessage(content\u001b[38;5;241m=\u001b[39mmodel_result\u001b[38;5;241m.\u001b[39mcontent, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py:513\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[0;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     cancellation_token\u001b[38;5;241m.\u001b[39mlink_future(future)\n\u001b[0;32m--> 513\u001b[0m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_beta_client:\n\u001b[1;32m    515\u001b[0m     result \u001b[38;5;241m=\u001b[39m cast(ParsedChatCompletion[Any], result)\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/openai/resources/chat/completions.py:1720\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1718\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1719\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1722\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1723\u001b[0m             {\n\u001b[1;32m   1724\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1725\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1726\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1727\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1728\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1729\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1736\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1737\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1738\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1739\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1740\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1741\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1742\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1743\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1747\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1748\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1749\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1750\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1751\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1752\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1753\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1754\u001b[0m             },\n\u001b[1;32m   1755\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1756\u001b[0m         ),\n\u001b[1;32m   1757\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1758\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1759\u001b[0m         ),\n\u001b[1;32m   1760\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1761\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1762\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1763\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/openai/_base_client.py:1849\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1837\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1846\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1847\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1848\u001b[0m     )\n\u001b[0;32m-> 1849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/openai/_base_client.py:1543\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1541\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1544\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1545\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1546\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1547\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1548\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1549\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hecbench-roofline/lib/python3.11/site-packages/openai/_base_client.py:1644\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1643\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1647\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1648\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1653\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}"
     ]
    }
   ],
   "source": [
    "gatheredData = ''\n",
    "\n",
    "for idx, target in enumerate(cudaKernels):\n",
    "    targetName = target['basename']\n",
    "    kernelNames = target['kernelNames']\n",
    "    kernels = target['kernels']\n",
    "\n",
    "    if len(kernelNames) == 0:\n",
    "        print(f'{targetName} has no found kernels -- skipping')\n",
    "        continue\n",
    "\n",
    "    # for now let's just stop early so we don't waste all our credits\n",
    "    if idx > 1:\n",
    "        break\n",
    "\n",
    "    for kernel in kernelNames:\n",
    "        kernelSrcCode = kernels[kernel]\n",
    "\n",
    "        for sysMsg in systemMessages:\n",
    "            for model in models:\n",
    "                for temp in temps:\n",
    "                    # wait 1 second between invocations, we don't want to get cloudflare banned, again...\n",
    "                    time.sleep(1.0)\n",
    "                    result = await ask_llm_for_roofline_classification(model, sysMsg, kernelSrcCode, temp)\n",
    "                    jsonLResult = chat_history_to_json_line(result)\n",
    "                    gatheredData += f'{jsonLResult}\\n'\n",
    "                    write_output_file(outputFile, gatheredData)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatheredDataJSONL = []\n",
    "with open(outputFile, 'r') as fp:\n",
    "    samples = fp.readlines()\n",
    "    for sample in samples:\n",
    "        sampleData = json.loads(sample)\n",
    "        gatheredDataJSONL.append(sampleData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m gatheredDataJSONL:\n\u001b[1;32m      3\u001b[0m     response \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompute\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBandwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalanced\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check that all the responses are of the desired string\n",
    "for sample in gatheredDataJSONL:\n",
    "    response = sample['messages'][2]['content']\n",
    "    assert response in ['Compute', 'Bandwidth', 'Balanced']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecbench-roofline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
