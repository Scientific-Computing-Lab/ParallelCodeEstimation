{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autogen version: 0.4.9.1\n"
     ]
    }
   ],
   "source": [
    "from roofline_survey_utils import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in the CSV file\n",
    "\n",
    "#resultsCSV = \n",
    "#\n",
    "#df = pd.read_csv(resultsCSV, quotechar='\\\"')\n",
    "#\n",
    "## let's clean the responses\n",
    "#\n",
    "#df['isBB'] = df['answer'].apply(lambda x: True if x == 'Bandwidth' else False)\n",
    "#\n",
    "#df['cleanResponse'] = df['llmResponse'].apply(cleanup_responses)\n",
    "#\n",
    "#df['isLLMCorrect'] = df['answer'] == df['cleanResponse']\n",
    "#\n",
    "## add some columns for ease-of-calculations\n",
    "#df['actual'] = df['answer'].apply(lambda x: 1 if x == 'Bandwidth' else 2)\n",
    "#df['predicted'] = df['cleanResponse'].apply(lambda x: 1 if x == 'Bandwidth' else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanup_responses(x):\n",
    "    #print('input:', x)\n",
    "    if not (str(x) == '<NA>'):\n",
    "        matches = re.finditer(r'([bB]andwidth|[cC]ompute)', x, re.MULTILINE)\n",
    "        matches = [m for m in matches]\n",
    "        if len(matches) > 1:\n",
    "            # just take the last match\n",
    "            print('\\tMore than 1 match, taking last one!')\n",
    "            matches = [matches[-1]]\n",
    "        else:\n",
    "            assert len(matches) == 1\n",
    "        for match in matches:\n",
    "            m = match.group()\n",
    "            return m.title()\n",
    "\n",
    "    print(f'returning NA for [{x}]')\n",
    "    assert False, \"this should never be reached!\"\n",
    "    return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics_of_df(df):\n",
    "    accuracy = accuracy_score(df['actual'], df['predicted'])\n",
    "    f1 = f1_score(df['actual'], df['predicted'], average='macro')\n",
    "    mcc = matthews_corrcoef(df['actual'], df['predicted'])\n",
    "    return (accuracy, f1, mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_csv(csvFile):\n",
    "    useCOT = False\n",
    "    hasLogProbs = False\n",
    "    if 'COT-' in csvFile:\n",
    "        useCOT = True\n",
    "    else:\n",
    "        assert 'simple-' in csvFile, \"Don't recognize input CSV file\"\n",
    "\n",
    "    if '-withLogProbs-' in csvFile:\n",
    "        hasLogProbs = True\n",
    "\n",
    "    regex = r\"(?<=-inference-results-).*(?=.csv)\"\n",
    "\n",
    "    matches = re.finditer(regex, csvFile, re.MULTILINE)\n",
    "\n",
    "    matches = [match for match in matches]\n",
    "    assert len(matches) == 1\n",
    "\n",
    "    modelName =  matches[0].group()\n",
    "    #modelName = csvFile[len(f\"{'COT' if useCOT else 'simple'}-{'withLogProbs' if hasLogProbs else ''}-inference-results-\"):-4]\n",
    "\n",
    "    df = pd.read_csv(csvFile, quotechar='\\\"')\n",
    "    \n",
    "    # let's just drop the failed cases for now\n",
    "    df = df.dropna(subset=['llmResponse'])\n",
    "\n",
    "    # do some response cleanup for returned strings that have more than 1 token\n",
    "    df['llmResponse'] = df['llmResponse'].apply(cleanup_responses)\n",
    "    # check if the LLM produced the correct answer\n",
    "    df['isLLMCorrect'] = df.apply(lambda x: x['answer'] == x['llmResponse'], axis=1)\n",
    "    \n",
    "    return (df, modelName, useCOT, hasLogProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(csvFiles):\n",
    "\n",
    "    dfStats = pd.DataFrame()\n",
    "\n",
    "    # each CSV file will get it's own row in the stats table\n",
    "    for csvName in csvFiles:\n",
    "        print('Calculating metrics for ', csvName)\n",
    "        csvDF, modelName, useCOT, hasLogProbs= read_results_csv(csvName)\n",
    "        print('Read CSV complete for model:', modelName)\n",
    "\n",
    "        summDict = {}\n",
    "        summDict['Model Name'] = [modelName]\n",
    "        summDict['Uses COT'] = [useCOT]\n",
    "        summDict['Has Log Probs?'] = [hasLogProbs]\n",
    "        summDict['Number of Samples'] = csvDF.shape[0]\n",
    "\n",
    "\n",
    "        # add some columns for ease-of-calculations\n",
    "        csvDF['actual'] = csvDF['answer'].apply(lambda x: 1 if x == 'Bandwidth' else 2)\n",
    "        csvDF['predicted'] = csvDF['llmResponse'].apply(lambda x: 1 if x == 'Bandwidth' else 2)\n",
    "\n",
    "        exampleCounts = list(csvDF['numExamples'].unique())\n",
    "\n",
    "        for numExamples in exampleCounts:\n",
    "            # get the samples that have this example count\n",
    "            subDF = csvDF[csvDF['numExamples'] == numExamples].reset_index(drop=True)\n",
    "            subMetrics = calc_metrics_of_df(subDF)\n",
    "            summDict[f'{numExamples}-shot (ACC, F1, MCC)'] = f\"({round(100.0*subMetrics[0],2)}, {round(100.0*subMetrics[1],2)}, {round(100.0*subMetrics[2],2)})\"\n",
    "\n",
    "\n",
    "        jointMetrics = calc_metrics_of_df(csvDF)\n",
    "\n",
    "        summDict['Joint Acc'] = round(100.0*jointMetrics[0],2)\n",
    "        summDict['Joint F1'] = round(100.0*jointMetrics[1],2)\n",
    "        summDict['Joint MCC'] = round(100.0*jointMetrics[2],2)\n",
    "\n",
    "        summDF = pd.DataFrame.from_dict(summDict)\n",
    "        dfStats = pd.concat([dfStats, summDF], ignore_index=True)\n",
    "\n",
    "\n",
    "    return dfStats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for  ./simple-withLogProbs-inference-results-gpt-4o-mini.csv\n",
      "Read CSV complete for model: gpt-4o-mini\n",
      "Calculating metrics for  ./COT-withLogProbs-inference-results-gpt-4o-mini.csv\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "\tMore than 1 match, taking last one!\n",
      "Read CSV complete for model: gpt-4o-mini\n",
      "    Model Name  Uses COT  Has Log Probs?  Number of Samples  \\\n",
      "0  gpt-4o-mini     False            True                 54   \n",
      "1  gpt-4o-mini      True            True                 54   \n",
      "\n",
      "   2-shot (ACC, F1, MCC)  4-shot (ACC, F1, MCC)  8-shot (ACC, F1, MCC)  \\\n",
      "0  (100.0, 100.0, 100.0)  (100.0, 100.0, 100.0)  (100.0, 100.0, 100.0)   \n",
      "1  (100.0, 100.0, 100.0)  (100.0, 100.0, 100.0)  (100.0, 100.0, 100.0)   \n",
      "\n",
      "   Joint Acc  Joint F1  Joint MCC  \n",
      "0      100.0     100.0      100.0  \n",
      "1      100.0     100.0      100.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "csvFiles = list(glob.glob('./*.csv'))\n",
    "\n",
    "stats = calculate_metrics(csvFiles)\n",
    "\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecbench-roofline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
