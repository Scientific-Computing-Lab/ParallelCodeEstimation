{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient, OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage, SystemMessage, AssistantMessage\n",
    "from autogen_core.model_context import UnboundedChatCompletionContext\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max SP TFLOP/s with FMA 25.068\n",
      "Max DP TFLOP/s with FMA 0.392\n",
      "Max SP TFLOP/s w/out FMA 12.534\n",
      "Max DP TFLOP/s w/out FMA 0.196\n",
      "Max TINTOP/s 12.534\n"
     ]
    }
   ],
   "source": [
    "# GPU specs\n",
    "\n",
    "# you can get this from deviceQuery\n",
    "gpuName = 'NVIDIA RTX 3080'\n",
    "\n",
    "# you can call nvidia-smi -i 0 -q to see what the clock is set to \n",
    "# you can also set the clock with nvidia-smi -lgc 1440,1440 for consistent measurements\n",
    "# vendor specs show the base clock\n",
    "baseClockHz = 1.440e9\n",
    "\n",
    "# find these values here: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions\n",
    "SPinstPerCyclePerSM = 128\n",
    "DPinstPerCyclePerSM = 2\n",
    "intInstPerCyclePerSM = 64\n",
    "\n",
    "# find this in deviceQuery or GPU vendor specs\n",
    "numSMs = 68\n",
    "\n",
    "# we always assume you're doing FMA -- we should add another roofline for non-FMA\n",
    "numFMAopPerInst = 2\n",
    "\n",
    "# conversion multiplier\n",
    "tflopPerflop = 1e-12\n",
    "\n",
    "# get this from your GPU vendor specs, mine was 760.3 GB/s\n",
    "maxBandwidthTBPerSec = 0.7603\n",
    "\n",
    "spOPMaxPerfTFLOP = SPinstPerCyclePerSM * numSMs * baseClockHz * numFMAopPerInst * tflopPerflop\n",
    "dpOPMaxPerfTFLOP = DPinstPerCyclePerSM * numSMs * baseClockHz * numFMAopPerInst * tflopPerflop\n",
    "intOPMaxPerfTFLOP = intInstPerCyclePerSM * numSMs * baseClockHz * numFMAopPerInst * tflopPerflop\n",
    "\n",
    "spOPMaxPerfTFLOP_noFMA = spOPMaxPerfTFLOP / 2\n",
    "dpOPMaxPerfTFLOP_noFMA = dpOPMaxPerfTFLOP / 2\n",
    "\n",
    "print('Max SP TFLOP/s with FMA', round(spOPMaxPerfTFLOP, 3))\n",
    "print('Max DP TFLOP/s with FMA', round(dpOPMaxPerfTFLOP, 3))\n",
    "print('Max SP TFLOP/s w/out FMA', round(spOPMaxPerfTFLOP_noFMA, 3))\n",
    "print('Max DP TFLOP/s w/out FMA', round(dpOPMaxPerfTFLOP_noFMA, 3))\n",
    "print('Max TINTOP/s', round(intOPMaxPerfTFLOP, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Gathered Data CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2970, 17)\n"
     ]
    }
   ],
   "source": [
    "dtypes={'Kernel Name':'string', \n",
    "        'traffic':np.float64,\n",
    "        'dpAI':np.float64,\n",
    "        'spAI':np.float64,\n",
    "        'dpPerf':np.float64,\n",
    "        'spPerf':np.float64,\n",
    "        'xtime':np.float64,\n",
    "        'Block Size': 'string',\n",
    "        'Grid Size': 'string',\n",
    "        'device': 'string',\n",
    "        \"intops\": np.float64, \n",
    "        \"intPerf\" : np.float64,\n",
    "        \"intAI\": np.float64,\n",
    "        'targetName': 'string',\n",
    "        'exeArgs': 'string',\n",
    "        'kernelName': 'string',\n",
    "        }\n",
    "\n",
    "# we need to gather more data for this dataset\n",
    "df = pd.read_csv('../roofline-data-new.csv', quotechar='\"', dtype=dtypes)\n",
    "\n",
    "# if we're loading old data that didn't gather intops\n",
    "#df = pd.read_csv('../roofline-data-OLD-only-cuda.csv', quotechar='\"', dtype=dtypes)\n",
    "#df['intops'] = 0\n",
    "#df['intPerf'] = 0\n",
    "#df['intAI'] = 0\n",
    "\n",
    "df['language'] = df['targetName'].apply(lambda x: 'CUDA' if '-cuda' in x else 'OMP')\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Kernel Name           targetName  \\\n",
      "0  AESEncrypt(uchar4 *, const uchar4 *, const uch...             aes-cuda   \n",
      "1  AIDW_Kernel(const float *, const float *, cons...            aidw-cuda   \n",
      "2  AIDW_Kernel_Tiled(const float *, const float *...            aidw-cuda   \n",
      "3  BP_queens_root_dfs(int, unsigned int, int, con...          nqueen-cuda   \n",
      "4  BezierGPU(const XYZ *, XYZ *, int, int, int, int)  bezier-surface-cuda   \n",
      "\n",
      "                                             exeArgs        xtime  \n",
      "0                  100 0 ../urng-sycl/URNG_Input.bmp     549600.0  \n",
      "1                                           10 1 100    3713056.0  \n",
      "2                                           10 1 100    2815680.0  \n",
      "3                                           15 7 100  154966912.0  \n",
      "4  ../face-cuda/Face.pgm ../face-cuda/info.txt .....    4395488.0  \n",
      "(1306, 4)\n",
      "          Kernel Name  traffic  dpAI  spAI  dpPerf  spPerf  xtime  Block Size  \\\n",
      "language                                                                        \n",
      "CUDA              757      757   757   757     757     757    757         757   \n",
      "OMP               621      621   621   621     621     621    621         621   \n",
      "\n",
      "          Grid Size  device  intops  intPerf  intAI  targetName  exeArgs  \\\n",
      "language                                                                   \n",
      "CUDA            757     757     757      757    757         757      757   \n",
      "OMP             621     621     621      621    621         621      621   \n",
      "\n",
      "          kernelName  \n",
      "language              \n",
      "CUDA             757  \n",
      "OMP              621  \n",
      "(1378, 17)\n"
     ]
    }
   ],
   "source": [
    "# because a lot of these kernels were sampled twice, let's drop the first sample (which typically runs for longer than the second sample)\n",
    "#grouped = df.groupby(by=['Kernel Name', 'kernelName', 'targetName', 'exeArgs'])['xtime'].min().reset_index()\n",
    "grouped = df.groupby(by=['Kernel Name', 'targetName', 'exeArgs'])['xtime'].min().reset_index()\n",
    "\n",
    "print(grouped.head())\n",
    "print(grouped.shape)\n",
    "\n",
    "df = df.merge(grouped, on=list(grouped.columns), how='inner')\n",
    "# it turns out when we give REGEX to 'ncu' to capture kernels, some kernel names have\n",
    "# extra characters that also get captured. e.g: AIDW_Kernel and AIDW_Kernel_Tiled both match for the former\n",
    "# so to deal with this we mainly filter by the 'Kernel Name'  instead of 'kernelName'\n",
    "#df = df.drop_duplicates(subset=['Kernel Name', 'kernelName', 'targetName', 'exeArgs', 'language'])\n",
    "\n",
    "counts = df.groupby(['language']).count()\n",
    "print(counts)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1378, 17)\n",
      "Index(['Kernel Name', 'traffic', 'dpAI', 'spAI', 'dpPerf', 'spPerf', 'xtime',\n",
      "       'Block Size', 'Grid Size', 'device', 'intops', 'intPerf', 'intAI',\n",
      "       'targetName', 'exeArgs', 'kernelName', 'language'],\n",
      "      dtype='object')\n",
      "Kernel Name    string[python]\n",
      "traffic               float64\n",
      "dpAI                  float64\n",
      "spAI                  float64\n",
      "dpPerf                float64\n",
      "spPerf                float64\n",
      "xtime                 float64\n",
      "Block Size     string[python]\n",
      "Grid Size      string[python]\n",
      "device         string[python]\n",
      "intops                float64\n",
      "intPerf               float64\n",
      "intAI                 float64\n",
      "targetName     string[python]\n",
      "exeArgs        string[python]\n",
      "kernelName     string[python]\n",
      "language               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's drop rows with NULL values and fix exeArgs that got a NaN value because they didn't have exeArgs\n",
    "\n",
    "These were executions that yielded no performance counter data (i.e: they didn't do single/double precision floating point operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1378, 17)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Kernel Name'].notna()] \n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# let's also replace exeArgs that are NaN with ''\n",
    "df['exeArgs'] = df['exeArgs'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Performance Data\n",
    "\n",
    "Here we scale down the `spPerf` and `dpPerf` columns to be on the scale of 1e11 (like how it's done in `ncu`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale everything to be in TFLOP/s\n",
    "df['dpPerf'] = df['dpPerf']*(1e-12)\n",
    "df['spPerf'] = df['spPerf']*(1e-12)\n",
    "df['intPerf'] = df['intPerf']*(1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Kernel Name', 'traffic', 'dpAI', 'spAI', 'dpPerf', 'spPerf', 'xtime',\n",
      "       'Block Size', 'Grid Size', 'device', 'intops', 'intPerf', 'intAI',\n",
      "       'targetName', 'exeArgs', 'kernelName', 'language'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique executables sampled: 548\n",
      "Total unique kernel executions recorded: 1309\n",
      "Number of kernels with no arithmetic intensity (AI) 556 (42.48%)\n",
      "Number of non-zero AI single-precision kernels recorded: 645 (49.27%)\n",
      "Number of non-zero AI double-precision kernels recorded: 216 (16.5%)\n",
      "Number of non-zero Perf single-precision kernels recorded: 645 (49.27%)\n",
      "Number of non-zero Perf double-precision kernels recorded: 216 (16.5%)\n",
      "Number of non-zero Perf intop kernels recorded: 1308 (99.92%)\n",
      "Number of non-zero AI intop kernels recorded: 1308 (99.92%)\n"
     ]
    }
   ],
   "source": [
    "# how many different kernel invocations did we capture?\n",
    "totalExes = df.groupby(['targetName', 'device', 'exeArgs']).ngroups\n",
    "print(f'Total unique executables sampled: {totalExes}')\n",
    "\n",
    "totalKernels = df.groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "print(f'Total unique kernel executions recorded: {totalKernels}')\n",
    "\n",
    "zeroAIKernels = df[(df['spAI'] == 0.0) & (df['dpAI'] == 0.0)].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "print(f'Number of kernels with no arithmetic intensity (AI) {zeroAIKernels} ({round(100*zeroAIKernels/totalKernels, 2)}%)')\n",
    "\n",
    "numSPKernels = df[df['spAI'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "numDPKernels = df[df['dpAI'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "\n",
    "print(f'Number of non-zero AI single-precision kernels recorded: {numSPKernels} ({round(100*numSPKernels/totalKernels, 2)}%)')\n",
    "print(f'Number of non-zero AI double-precision kernels recorded: {numDPKernels} ({round(100*numDPKernels/totalKernels, 2)}%)')\n",
    "\n",
    "numSPKernels = df[df['spPerf'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "numDPKernels = df[df['dpPerf'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "\n",
    "print(f'Number of non-zero Perf single-precision kernels recorded: {numSPKernels} ({round(100*numSPKernels/totalKernels, 2)}%)')\n",
    "print(f'Number of non-zero Perf double-precision kernels recorded: {numDPKernels} ({round(100*numDPKernels/totalKernels, 2)}%)')\n",
    "\n",
    "numIntPerfKernels = df[df['intPerf'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "numIntAIKernels = df[df['intAI'] > 0.0].groupby(['Kernel Name', 'kernelName', 'targetName', 'device', 'Block Size', 'Grid Size', 'exeArgs']).ngroups\n",
    "\n",
    "\n",
    "print(f'Number of non-zero Perf intop kernels recorded: {numIntPerfKernels} ({round(100*numIntPerfKernels/totalKernels, 2)}%)')\n",
    "print(f'Number of non-zero AI intop kernels recorded: {numIntAIKernels} ({round(100*numIntAIKernels/totalKernels, 2)}%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Block and Grid Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Kernel Name       traffic      dpAI  \\\n",
      "0  haccmk_kernel(int, int, const float *, const f...  7.156462e+07  0.000000   \n",
      "1  bit_rev_permutation(long *, const long *, unsi...  4.585366e+09  0.000000   \n",
      "2  bit_rev_permutation_z(long *, const long *, un...  6.496454e+09  0.000000   \n",
      "3  initial_value(unsigned int, double, double, do...  2.528404e+10  9.317279   \n",
      "4  solve(unsigned int, double, double, double, do...  5.187426e+11  0.563661   \n",
      "\n",
      "          spAI    dpPerf    spPerf      xtime    Block Size      Grid Size  \\\n",
      "0  1868.406255  0.000000  0.133712  2990528.0   (256, 1, 1)      (4, 1, 1)   \n",
      "1     0.000000  0.000000  0.000000     3936.0  (1024, 1, 1)      (1, 1, 1)   \n",
      "2     0.000000  0.000000  0.000000     4512.0    (32, 1, 1)      (2, 1, 1)   \n",
      "3     0.409507  0.235578  0.010354  6480864.0   (256, 1, 1)  (65536, 1, 1)   \n",
      "4     0.000000  0.292395  0.000000   516416.0   (256, 1, 1)  (65536, 1, 1)   \n",
      "\n",
      "                    device       intops   intPerf       intAI  \\\n",
      "0  NVIDIA GeForce RTX 3080  105854832.0  0.035397  494.611767   \n",
      "1  NVIDIA GeForce RTX 3080      14336.0  0.003642    0.794326   \n",
      "2  NVIDIA GeForce RTX 3080      11648.0  0.002582    0.397380   \n",
      "3  NVIDIA GeForce RTX 3080  855707646.0  0.132036    5.222111   \n",
      "4  NVIDIA GeForce RTX 3080  637562878.0  1.234592    2.379970   \n",
      "\n",
      "        targetName    exeArgs             kernelName language  blockSz  gridSz  \n",
      "0      haccmk-cuda       1000          haccmk_kernel     CUDA      256       4  \n",
      "1  bitpermute-cuda        100    bit_rev_permutation     CUDA     1024       1  \n",
      "2  bitpermute-cuda        100  bit_rev_permutation_z     CUDA       32       2  \n",
      "3        heat-cuda  4096 1000          initial_value     CUDA      256   65536  \n",
      "4        heat-cuda  4096 1000                  solve     CUDA      256   65536  \n"
     ]
    }
   ],
   "source": [
    "# because the sizes are in 3D, let's convert them to 1D by multiplying them\n",
    "\n",
    "\n",
    "def strTupleTo1D(strTuple):\n",
    "    finds = re.findall(r'\\d+', strTuple)\n",
    "    nums = [int(find) for find in finds]\n",
    "\n",
    "    assert len(nums) == 3\n",
    "\n",
    "    mult = 1\n",
    "    for num in nums:\n",
    "        mult = mult*num\n",
    "    return mult\n",
    "\n",
    "\n",
    "df['blockSz'] = df['Block Size'].apply(strTupleTo1D).astype(int)\n",
    "df['gridSz'] = df['Grid Size'].apply(strTupleTo1D).astype(int)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the JSON files with the scraped kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('simple-scraped-kernels-CUDA-pruned.json', 'r') as file:\n",
    "    scrapedCUDA = json.load(file)\n",
    "\n",
    "with open('simple-scraped-kernels-OMP-pruned.json', 'r') as file:\n",
    "    scrapedOMP = json.load(file)\n",
    "\n",
    "\n",
    "scrapedCodes = scrapedCUDA + scrapedOMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match up the kernels with their JSON and save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_history_to_json_line(ctxMessages:list):\n",
    "    jsonDict = {'messages':[]}\n",
    "    for msg in ctxMessages:\n",
    "        if type(msg) == SystemMessage:\n",
    "            role = 'system'\n",
    "        elif type(msg) == UserMessage:\n",
    "            role = 'user'\n",
    "        elif type(msg) == AssistantMessage:\n",
    "            role = 'assistant'\n",
    "        else:\n",
    "            assert False, f'Unknown message type: {type(msg)} of {msg}'\n",
    "        content = msg.content\n",
    "\n",
    "        jsonDict['messages'].append({'role':role, 'content':content})\n",
    "\n",
    "    return json.dumps(jsonDict, allow_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demangle_omp_kernel_name(mangledName):\n",
    "\n",
    "    regex = r'_(?:[^_]+_){4}(.*)(_l[\\d]+)'\n",
    "    matches = re.finditer(regex, mangledName, re.MULTILINE)\n",
    "\n",
    "    matches = [i for i in matches]\n",
    "    assert len(matches) == 1\n",
    "\n",
    "    cleanName = ''\n",
    "    for match in matches:\n",
    "        groups = match.groups()\n",
    "        assert len(groups) == 2\n",
    "        cleanName = groups[0]\n",
    "        break\n",
    "\n",
    "\n",
    "    filterCommand = f'llvm-cxxfilt {cleanName}'\n",
    "    \n",
    "    demangleResult = subprocess.run(filterCommand, shell=True, timeout=5, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "    assert demangleResult.returncode == 0\n",
    "    \n",
    "    demangled = demangleResult.stdout.decode('UTF-8').strip()\n",
    "\n",
    "    return demangled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this system message combines the device information and the non-detailed few-shot\n",
    "# examples. We need some few-shot breif examples of the output so that the model \n",
    "# knows how to respond.\n",
    "systemMessage = '''You are a GPU performance analysis expert that classifies kernels into Arithmetic Intensity Roofline model categories based on their source code characteristics. Your task is to provide one of the following performance boundedness classifications: Compute or Bandwidth.  A kernel is considered Compute bound if its performance is primarily limited by the number of operations it performs, and Bandwidth bound if its performance is primarily limited by the rate at which data can be moved between memory and processing units.\n",
    "\n",
    "Provide only one word as your response, chosen from the set: ['Compute', 'Bandwidth'].\n",
    "**Examples:**\n",
    "**Example 1:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000000 {\n",
    "  a[i] = a[i] + b[i];\n",
    "}\n",
    "```\n",
    "Response: Compute\n",
    "\n",
    "**Example 2:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 10 {\n",
    "  load_data(large_array);   //loads from large memory\n",
    "  process_data(large_array); //processes data\n",
    "  store_data(large_array);  //stores back to memory\n",
    "}\n",
    "```\n",
    "Response: Bandwidth\n",
    "\n",
    "\n",
    "Now, analyze the following source codes for the requested CUDA or OpenMP (OMP) target offload kernel of the specified hardware.'''\n",
    "\n",
    "\n",
    "'''\n",
    "**Example 3:**\n",
    "```\n",
    "Kernel Source Code (simplified):\n",
    "for i = 0 to 1000 {\n",
    "  vector_add(a,b,c);   //process data in situ\n",
    "}\n",
    "//Some smaller data movement but mostly compute.\n",
    "```\n",
    "Response: Compute\n",
    "'''\n",
    "\n",
    "\n",
    "def make_kernel_info_message(device, exeArgs, peakPerfGFLOPs, memBandwidthGBs, kernelName, blockSz, gridSz, language):\n",
    "    assert kernelName != ''\n",
    "\n",
    "    if language == 'OMP':\n",
    "      cleanKName = demangle_omp_kernel_name(kernelName)\n",
    "      assert cleanKName != ''\n",
    "      beginPart = f'Classify the {language} kernel in function [{cleanKName}] as Bandwidth or Compute bound.'\n",
    "    else:\n",
    "      # if were prompting for a CUDA code\n",
    "      cleanKName = kernelName\n",
    "      beginPart = f'Classify the {language} kernel called [{cleanKName}] as Bandwidth or Compute bound.'\n",
    "\n",
    "    builtPrompt = f'{beginPart} The system it will execute on is a [{device}] with a peak performance of {round(peakPerfGFLOPs,2)} GFLOP/s and a max bandwidth of {round(memBandwidthGBs,2)} GB/s. The block and grid sizes of the invoked kernel are {blockSz} and {gridSz}, respectively. The executable running this kernel is launched with '\n",
    "\n",
    "    if exeArgs == '':\n",
    "      builtPrompt += 'no command line arguments.'\n",
    "    else:\n",
    "      builtPrompt += f'the following command line arguments: [{exeArgs}].'\n",
    "\n",
    "    builtPrompt += ' Below is the source code containing the kernel definition and other source code for the executable.'\n",
    "\n",
    "    return builtPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the kernel name is from the \"Kernel Name\" column of the dataframe\n",
    "async def make_chat_history(kernel_info, kernelCode):\n",
    "\n",
    "    sys_msg = SystemMessage(content=systemMessage)\n",
    "    kernel_info_msg = UserMessage(source='User', content=kernel_info)\n",
    "    code_msg = UserMessage(source='User', content=f'```{kernelCode}```')\n",
    "    context = UnboundedChatCompletionContext(initial_messages=[sys_msg, kernel_info_msg, code_msg])\n",
    "\n",
    "    return context\n",
    "    #messages = await context.get_messages()\n",
    "    #return messages\n",
    "\n",
    "async def make_chat_history_with_answer(kernel_info, kernelCode, answer):\n",
    "\n",
    "    sys_msg = SystemMessage(content=systemMessage)\n",
    "    kernel_info_msg = UserMessage(source='User', content=kernel_info)\n",
    "    code_msg = UserMessage(source='User', content=f'```{kernelCode}```')\n",
    "    assis_msg = AssistantMessage(source='assistant', content=f'{answer}')\n",
    "    context = UnboundedChatCompletionContext(initial_messages=[sys_msg, kernel_info_msg, code_msg, assis_msg])\n",
    "\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeToFile(filename, lines):\n",
    "    # going to overwrite the whole file each time\n",
    "    # it's redundant but the file wont be that large\n",
    "    # so the speed doesn't matter\n",
    "    with open(filename, 'w') as jsonLFile:\n",
    "        jsonLFile.write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please create a file called '.llm-api-key' with your api key and no newline characters\n",
    "with open('./.llm-api-key', 'r') as file:\n",
    "    LLM_API_KEY=file.read().strip()\n",
    "\n",
    "with open('./.openrouter-api-key', 'r') as file:\n",
    "    OPENROUTER_API_KEY=file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def ask_llm_for_roofline_classification(chatHistory, useAzure=False, temp=1.0, timeout=60):\n",
    "\n",
    "    model_client = None\n",
    "    if useAzure:\n",
    "        model_client = AzureOpenAIChatCompletionClient(\n",
    "                model='gpt-4o-mini',\n",
    "\n",
    "                azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "                #azure_endpoint='https://galor-m6d0ej1n-eastus2.cognitiveservices.azure.com',\n",
    "                azure_deployment='gpt-4o-mini',\n",
    "                api_key=LLM_API_KEY,\n",
    "                timeout=timeout,\n",
    "                temperature=temp,\n",
    "                api_version='2024-08-01-preview',\n",
    "                #api_version='2025-01-01-preview',\n",
    "        )\n",
    "    else:\n",
    "        model_client = OpenAIChatCompletionClient(\n",
    "                model='openai/gpt-4o-mini',\n",
    "                base_url='https://openrouter.ai/api/v1',\n",
    "                api_key=OPENROUTER_API_KEY,\n",
    "                timeout=timeout,\n",
    "                temperature=temp,\n",
    "                model_info = {'vision':False, 'function_calling':True, 'json_output':True, 'model_family':'unknown'}\n",
    "        )\n",
    "\n",
    "            #model_info = {'vision':False, 'function_calling':True, 'json_output':True, 'model_family':'unknown'}\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        model_client=model_client,\n",
    "        model_context=chatHistory\n",
    "    )\n",
    "\n",
    "    await agent.run()\n",
    "    return await agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1378/1378 [00:23<00:00, 58.37it/s]\n"
     ]
    }
   ],
   "source": [
    "jsonLLines = ''\n",
    "\n",
    "peakPerfGFLOPs = spOPMaxPerfTFLOP * 1e3\n",
    "memBandwidthGBs = maxBandwidthTBPerSec * 1e3\n",
    "\n",
    "balancePointFLOPPerByte = spOPMaxPerfTFLOP / maxBandwidthTBPerSec\n",
    "\n",
    "# for each sample we got\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "    targetName = row['targetName']\n",
    "    kernelName = row['Kernel Name']\n",
    "    exeArgs = row['exeArgs']\n",
    "    blockSz = row['Block Size']\n",
    "    gridSz = row['Grid Size']\n",
    "    language = row['language']\n",
    "    device = row['device']\n",
    "    flopAI = row['spAI']\n",
    "\n",
    "    expectedAnswer = 'Bandwidth' if flopAI < balancePointFLOPPerByte else 'Compute'\n",
    "\n",
    "    for elem in scrapedCodes:\n",
    "        basename = elem['basename']\n",
    "        if basename == targetName:\n",
    "            kernelCode = list(elem['kernels'].values())[0]\n",
    "            assert kernelCode != ''\n",
    "            break\n",
    "\n",
    "\n",
    "    infoMsg = make_kernel_info_message(device, exeArgs, peakPerfGFLOPs, memBandwidthGBs, kernelName, blockSz, gridSz, language)\n",
    "    chatHist = await make_chat_history_with_answer(infoMsg, kernelCode, expectedAnswer)\n",
    "\n",
    "    #resultHist = await ask_llm_for_roofline_classification(chatHist, useAzure=True, temp=0.1, timeout=60)\n",
    "    resultStr = chat_history_to_json_line(await chatHist.get_messages())\n",
    "\n",
    "    jsonLLines = jsonLLines + resultStr + '\\n'\n",
    "    writeToFile('zero-shot-FULL-TRAIN-Dataset.jsonl', jsonLLines.rstrip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_split(jsonLFile, seed=1928):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    numLines = 0\n",
    "    with open(jsonLFile, 'r') as f:\n",
    "        numLines = sum(1 for _ in f)\n",
    "\n",
    "    # create an array and shuffle it, take the top 20 percent for test and leftover as train\n",
    "    shuffled = np.array(list(range(numLines)))\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    top80 = int(len(shuffled)*0.8)\n",
    "    bot20 = len(shuffled) - top80\n",
    "    print(f'Num elems in 20% {bot20}, num elems in 80% {top80}')\n",
    "    train, test = shuffled[:top80], shuffled[top80:]\n",
    "\n",
    "    \n",
    "    trainData = ''\n",
    "    testData = ''\n",
    "    with open(jsonLFile, 'r') as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            if i in train:\n",
    "                trainData += line.strip() + '\\n'\n",
    "            else:\n",
    "                testData += line.strip() + '\\n'\n",
    "\n",
    "    writeToFile(f'train-data-{top80}-1378.jsonl', trainData.rstrip())\n",
    "    writeToFile(f'test-data-{bot20}-1378.jsonl', testData.rstrip())\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num elems in 20% 276, num elems in 80% 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1378it [00:00, 75718.58it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_test_split('zero-shot-FULL-TRAIN-Dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_context_obj(jsonLStr):\n",
    "    lines = jsonLStr.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        pass\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecbench-roofline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
